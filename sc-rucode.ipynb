{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install reqs","metadata":{}},{"cell_type":"code","source":"!pip install gdown\n!pip install pytorch-lightning==1.7.7\n!pip install torch==1.11.0\n!pip install torchmetrics==0.10.0\n!pip install transformers==4.20.1\n!pip install pandas\n!pip install numpy\n!pip install sklearn\n!pip install wandb # Optional","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import packages","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.utils.data.dataset import Dataset\nfrom pytorch_lightning import LightningDataModule, seed_everything, Trainer\nfrom pytorch_lightning.core.module import LightningModule\nimport torchmetrics.functional as MF\n\nfrom collections import OrderedDict\n\nimport wandb\nfrom pytorch_lightning.loggers import WandbLogger\n\nimport re\n\nfrom transformers.optimization import Adafactor, AdafactorSchedule\nfrom transformers import T5ForConditionalGeneration, T5TokenizerFast, T5Config\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nimport gdown","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define datasets","metadata":{}},{"cell_type":"markdown","source":"## Dataset with labels","metadata":{}},{"cell_type":"code","source":"class SCDataset(Dataset):\n    \"\"\"Pytorch index dataset from Pandas DataFrame for conditional text generation, with labels provided.\n\n    Attributes\n    ----------\n    dataframe : DataFrame\n        Pandas DataFrame from which text is pulled\n    tokenizer : PreTrainedTokenizer\n        HuggingFace tokinizer\n    source_text : str\n        Column name for source text\n    target_text : str\n        Column name for target text\n    \"\"\"\n\n    def __init__(\n        self, dataframe, tokenizer, source_text, target_text\n    ):\n        \"\"\"Load tokenizer and source and target columns from DataFrame into memory.\n        Find maximum length of strings in each column.\n        \"\"\"\n\n        self.tokenizer = tokenizer\n        self.target_text = dataframe[target_text]\n        self.source_text = dataframe[source_text]\n        self.source_len = self.source_text.str.len().max()\n        self.summ_len = self.target_text.str.len().max()     \n\n    def __len__(self):\n        \"\"\"Get length of dataset.\n\n        Returns\n        -------\n        int\n            Length of dataset\n        \"\"\"\n\n        return len(self.target_text)\n\n    def __getitem__(self, index):\n        \"\"\"Get item from dataset by index.\n        1) Index training sample from source and target columns.\n        2) Basic cleaning of strings.\n        3) Add finetuning prompt (not necessary). \n        4) Encode both strings with tokenizer, with padding to maximum length.\n        5) Return attention masks, input ids (for target and source text) and clear target text.\n\n        Parameters\n        ----------\n        index : int\n\n        Returns\n        -------\n        source[\"input_ids\"] : LongTensor\n        source[\"attention_mask\"]: LongTensor\n        target[\"input_ids\"] : LongTensor\n        target[\"attention_mask\"]: LongTensor\n        target_text: str\n            Clean target text, for metric calculation\n        \"\"\"\n        \n        source_text = self.source_text.iloc[index]\n        target_text = self.target_text.iloc[index]\n\n        source_text = \" \".join(source_text.split())\n        target_text = \" \".join(target_text.split())\n\n        source_text = re.compile(r'[\\-\\˗\\֊\\‐\\‑\\‒\\–\\—\\⁻\\₋\\−\\﹣\\－]', re.UNICODE).sub('-', source_text)\n        target_text = re.compile(r'[\\-\\˗\\֊\\‐\\‑\\‒\\–\\—\\⁻\\₋\\−\\﹣\\－]', re.UNICODE).sub('-', target_text)\n\n        source = self.tokenizer.batch_encode_plus(\n            [\"Исправление ошибок: \"+source_text],\n            max_length=self.source_len,\n            pad_to_max_length=True,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\",\n        )\n        target = self.tokenizer.batch_encode_plus(\n            [target_text],\n            max_length=self.summ_len,\n            pad_to_max_length=True,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\",\n        )\n\n        return source[\"input_ids\"].squeeze(), source[\"attention_mask\"].squeeze(), target[\"input_ids\"].squeeze(), target[\"attention_mask\"].squeeze(), target_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset without labels","metadata":{}},{"cell_type":"code","source":"class SCPredictDataset(Dataset):\n    \"\"\"Pytorch index dataset from Pandas DataFrame for conditional text generation, without labels provided, suitible only for prediction\n\n    Attributes\n    ----------\n    dataframe : DataFrame\n        Pandas DataFrame from which text is pulled\n    tokenizer : PreTrainedTokenizer\n        HuggingFace tokinizer\n    source_text : str\n        Column name for source text\n    \"\"\"\n    def __init__(\n        self, dataframe, tokenizer, source_text\n    ):\n        self.tokenizer = tokenizer\n        self.source_text = dataframe[source_text]\n        self.source_len = self.source_text.str.len().max()\n        print('in dataset init')\n\n    def __len__(self):\n        \"\"\"Get length of dataset.\n\n        Returns\n        -------\n        len : int\n            Length of dataset\n        \"\"\"\n        return len(self.source_text)\n\n    def __getitem__(self, index):\n        \"\"\"Get item from dataset by index.\n        1) Index inference sample from source column.\n        2) Basic cleaning of string.\n        3) Add finetuning prompt (not necessary). \n        4) Encode string with tokenizer, with padding to maximum length.\n        5) Return attention mask, input ids (for source text).\n\n        Parameters\n        ----------\n        index : int\n\n        Returns\n        -------\n        source[\"input_ids\"] : LongTensor\n        source[\"attention_mask\"]: LongTensor\n        \"\"\"\n\n        source_text = self.source_text.iloc[index]\n        source_text = \" \".join(source_text.split())\n        source_text = re.compile(r'[\\-\\˗\\֊\\‐\\‑\\‒\\–\\—\\⁻\\₋\\−\\﹣\\－]', re.UNICODE).sub('-', source_text)\n\n        source = self.tokenizer.batch_encode_plus(\n            [\"Исправление ошибок: \"+source_text],\n            max_length=self.source_len,\n            pad_to_max_length=True,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\",\n        ) \n        return source[\"input_ids\"].squeeze(), source[\"attention_mask\"].squeeze()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training args","metadata":{}},{"cell_type":"code","source":"args = {}\nargs['workers'] = 0 # Memory and CPU load stability.\nargs['epochs'] = 1  # Model has been trained only on one epoch. My guess, what on 2 epochs I could achive around 0.67 ACC, but I didn't have enough time, to do so.\nargs['warmup'] = 0  # No warmup on 1 ecpoch training.\nargs['batch_size'] = 64 # ~12GB GPU RAM.\nargs['lr'] = 0.001 # Taken from T5 paper. Suitable for one epoch.\nargs['weight_decay'] = 5e-2 # Taken from T5 paper. Suitable for one epoch.\nargs['min_lr'] = 0.00001 # One epoch, so no scheduler, no need in that.\nargs['seed'] = 42 # Random seed for stability.\n\nclass Args:\n    def __init__(self, **entries):\n        self.__dict__.update(entries)\n        \nargs = Args(**args)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define model ","metadata":{}},{"cell_type":"code","source":"class SCModel(LightningModule):\n    \"\"\"Custom lightning module for model.\n\n    Attributes\n    ----------\n    args : Args\n        Args object\n    model : PreTrainedModel\n        HuggingFace text model\n    tokenizer : PreTrainedTokenizer\n        HuggingFace tokinizer\n    \"\"\"\n    def __init__(self, args, model, tokenizer):\n        \"\"\"Create module. Ignoring model and tokenizer hyp-es.\n        \"\"\"\n        super().__init__()\n        self.save_hyperparameters(ignore=[\"model\", \"tokenizer\"])\n        self.model = model\n        self.tokenizer = tokenizer\n        self.args = args\n        self.training_epoch_end = None\n        \n    def forward(self, batch):\n        \"\"\"Forward pass of HF text model.\n\n        Parameters\n        ----------\n        batch : tuple\n            Input batch\n\n        Returns\n        -------\n        torch.FloatTensor\n            Loss of model\n        \"\"\"\n        source_ids, source_mask, target_ids, target_mask, _ = batch # Unpack batch.\n\n        target_ids[target_ids[:, :] == self.tokenizer.pad_token_id] = -100 # Set padding tokens attention in target to -100, to not influence the loss.\n        \n        return self.model(\n            input_ids=source_ids,\n            attention_mask=source_mask,\n            labels=target_ids,\n            decoder_attention_mask=target_mask\n        )[0] #calculate loss of T5\n\n    def val_forward(self, batch):\n        \"\"\"Inference pass of HF text model.\n\n        Parameters\n        ----------\n        batch : tuple\n            Input batch\n\n        Returns\n        -------\n        list[str]\n            List of decoded tokens\n        \"\"\"      \n        source_ids = batch[0] # Unpack batch.\n        source_mask = batch[1] # Unpack batch.\n        generated_ids = self.model.generate(\n            input_ids = source_ids,\n            attention_mask = source_mask, \n            max_length=16, # Common maximum from train and public test datasets.\n            num_beams=10,# Kinda overkill, but still ok time for inference.\n            repetition_penalty=5.0, # Noticed many repetitions of punctuaction (!?-.,), so decided to increase that penalty.\n            length_penalty=1.0, # No need in high penalty, good length distribution.\n            early_stopping=True, # Ensure stability.\n            top_p=0.75 # Previously was on 0.99, decided to look at wider range of answers.\n        )\n        return [self.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids] # Decode without special tokens and excessive spaces.\n\n    def training_step(self, batch, batch_idx):\n        \"\"\"Training step.\n    \n        Parameters\n        ----------\n        batch : tuple\n            Input batch\n        batch_idx : int\n\n        Returns\n        -------\n        FloatTensor\n            Trainig loss\n        \"\"\"\n        loss = self.forward(batch)\n        self.log(\"train_loss\", loss) # Log trainig loss\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        \"\"\"Validation step.\n\n        Parameters\n        ----------\n        batch : tuple\n            Input batch\n        batch_idx : int\n        \"\"\"\n        loss = self.forward(batch)\n        self.log(\"val_loss\", loss)# Log validation loss\n\n    def test_step(self, batch, batch_idx):\n        \"\"\"Testing step.\n        Calculate losses and metrics.\n        Decided to calulate metrics only on test step to speedup training.\n\n        Parameters\n        ----------\n        batch : tuple\n            Input batch\n        batch_idx: int\n\n        Returns\n        -------\n        str\n            Predicted string\n        \"\"\"\n        y = batch[-1]\n        loss = self.forward(batch)\n        self.log(\"test_loss\", loss) # Log loss\n        preds = self.val_forward(batch) # Inference\n        self.log('acc',MF.classification.accuracy(torch.Tensor([1 if i == j and j != '' else 0 for (i,j) in zip(preds, y)]), torch.ones((len(preds))).long()), on_epoch = True) # Calculate CLS accuracy (like in leaderboard)\n        self.log('bleu',MF.bleu_score(preds, y), on_epoch = True) # Calculate BLEU score\n        self.log('cer',MF.char_error_rate(preds, y), on_epoch = True) # Calculate char error rate\n        self.log('wer',MF.word_error_rate(preds, y), on_epoch = True) # Calculate word error rate\n        \n        return preds\n    \n    \n    def predict_step(self, batch, batch_idx):\n        \"\"\"Inference step.\n        Predict corrected string.\n\n        Parameters\n        ----------\n        batch : tuple\n            Input batch\n        batch_idx: int\n\n        Returns\n        -------\n        str\n            Predicted string\n        \"\"\"\n        return self.val_forward(batch)\n    \n    \n    def configure_optimizers(self):\n        \"\"\"Configuring optimizers for PL module.\n        Using Adafactor with parameters as in original T5 paper.\n\n        No need in scheduler because of 1 epoch finetuning.\n        \"\"\"\n        optimizer = Adafactor(self.model.parameters(), scale_parameter=False, relative_step=False, lr=self.args.lr)\n        lr_scheduler = False #AdafactorSchedule(optimizer)\n\n        if lr_scheduler:\n            return {\n                \"optimizer\": optimizer,\n                \"lr_scheduler\": lr_scheduler,\n                \"monitor\": \"val_loss\",\n            }\n        else:\n            return {\"optimizer\": optimizer}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load and prepare data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('./data/train.csv') # Or your path.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['correct_text'] = df['correct_text'].str.replace(r'\\s+', ' ', regex=True) # Removing excessive spaces\ndf['corrupted_text'] = df['corrupted_text'].str.replace(r'\\s+', ' ', regex=True) # Removing excessive spaces","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_full, df_test = train_test_split(df, stratify=df['category'], test_size=0.01, random_state = args.seed) # train+val / test split. Model never sees 1% of data. ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train, df_val = train_test_split(df_full, stratify=df_full['category'], test_size=0.15, random_state = args.seed) # train/val split 85-15","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare HuggingFace ruT5-base model and tokinizer","metadata":{}},{"cell_type":"code","source":"dev = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using pretrained ruT5 from sber, t2t objective. Decided to use base model (not large) for lower speed and memory consumption.\ntokenizer = T5TokenizerFast.from_pretrained(\"sberbank-ai/ruT5-base\") \nt_model = T5ForConditionalGeneration.from_pretrained(\"sberbank-ai/ruT5-base\", return_dict = True).to(dev)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define datasets and loaders","metadata":{}},{"cell_type":"code","source":"train = SCDataset(df_train, tokenizer, 'corrupted_text', 'correct_text')\nval = SCDataset(df_val, tokenizer, 'corrupted_text', 'correct_text')\ntest = SCDataset(df_test, tokenizer, 'corrupted_text', 'correct_text')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train, batch_size=args.batch_size, shuffle=True, num_workers=args.workers, pin_memory=True)\nval_loader = DataLoader(val, batch_size=args.batch_size//2, shuffle=False, num_workers=args.workers, pin_memory=True) # Divide batch size on 2 to ensure stable memory consumption.\ntest_loader = DataLoader(test, batch_size=args.batch_size//2, shuffle=False, num_workers=args.workers, pin_memory=True) # Divide batch size on 2 to ensure stable memory consumption.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define PL Trainer","metadata":{}},{"cell_type":"code","source":"seed_everything(args.seed)\nmodel = SCModel(args, model = t_model, tokenizer = tokenizer)\ntrainer = Trainer(\n    max_epochs=args.epochs,\n    num_sanity_val_steps=0,\n    devices=1, # Trained on one GPU\n    accelerator=\"auto\", \n    logger=WandbLogger(project=\"t5ru-optimizing-nontok-batches\"), # I used Wandb to track training of models.\n    default_root_dir = './checkpoint',\n#     callbacks=[checkpoint_callback, lr_monitor_callback, early_stop_callback], # Optional\n    log_every_n_steps=20, # To speed up training\n    accumulate_grad_batches = 4 # Accumulating batches to speed up training.\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training loop","metadata":{}},{"cell_type":"code","source":"trainer.fit(model, train_loader, val_loader)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test loop","metadata":{}},{"cell_type":"code","source":"trainer.test(model, test_loader)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"#Downloading checkpoint from google drive\ngd_id = '1lmT1tCbX-s3MDxMhvO1NvXbMffeXpPLp'\ngdown.download(f'https://drive.google.com/uc?id={gd_id}', 'checkpoint.ckpt', quiet=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Prepare checkpoint to load in PL module. \nstate_dict = OrderedDict({\n    k.replace('model.', ''):v\n    for k,v in torch.load('./checkpoint.ckpt', map_location=dev)[\"state_dict\"].items()\n})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Defining clear T5 model (no weights).\nconfig = T5Config.from_pretrained(\n    \"sberbank-ai/ruT5-base\"\n)\ntokenizer = T5TokenizerFast.from_pretrained(\"sberbank-ai/ruT5-base\")\nt_model = T5ForConditionalGeneration(config).to(dev)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_batch_size = int(input(\"Размер батча для предсказания: \"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pred = pd.read_csv('./data/private_test.csv') # Or your path.\npred = SCPredictDataset(df_pred, tokenizer, 'corrupted_text')\npred_loader = DataLoader(pred, batch_size=pred_batch_size, shuffle=False, num_workers=0, pin_memory=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_everything(args.seed)\nmodel = SCModel(args, model = t_model, tokenizer = tokenizer)\nmodel.model.load_state_dict(state_dict) # Loading weights.\n\ntrainer = Trainer(\n    max_epochs=args.epochs,\n    num_sanity_val_steps=0,\n    devices=1,\n    accelerator=\"auto\"\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_df = trainer.predict(model, pred_loader, return_predictions=True) # Inference loop.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_df_df = pd.DataFrame([item for sublist in predictions_df for item in sublist]) # Convert list[list] to DataFrame.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_df_df.to_csv('sample.csv', index=False, header = False) # Save to csv.","metadata":{},"execution_count":null,"outputs":[]}]}